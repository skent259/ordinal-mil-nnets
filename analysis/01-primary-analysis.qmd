---
title: "01 Primary Analysis"
format: 
  html:
    fig-width: 8
    fig-height: 4
    code-fold: true
editor: visual
---

```{r, setup}
#| warning: false
#| message: false

library(tidyverse)
library(here)
library(glue)
library(ggbeeswarm)
library(ggfortify)
library(patchwork)

library(lme4)

# Read in data
df_metrics <- read_csv(here("results/test_metrics_from_experiments.csv"))
df_train_prog <- read_csv(here("results/train-val_progression_from_experiments.csv"))

# exp_list <- c("fgnet-1.0.1", "fgnet-1.0.2", "aes-2.1.0")
exp_list <- c("fgnet-1.0.1", "fgnet-1.0.2")

```

## Experiment

Factors used:

``` python
{
  "ordinal_method": [OrdinalType.CORAL, OrdinalType.CORN],
  "mil_method": [MILType.CAP_MI_NET, MILType.MI_NET, MILType.CAP_MI_NET_DS],
  "data_set_type": [DataSetType.FGNET],
  "data_set_name": [
      "fgnet_bag_wr=0.5_size=4_i=0_j=0",
      "fgnet_bag_wr=0.5_size=4_i=0_j=1",
      "fgnet_bag_wr=0.5_size=4_i=0_j=2",
      "fgnet_bag_wr=0.5_size=4_i=0_j=3",
      "fgnet_bag_wr=0.5_size=4_i=0_j=4",
      "fgnet_bag_wr=0.5_size=4_i=1_j=0",
      "fgnet_bag_wr=0.5_size=4_i=1_j=1",
      "fgnet_bag_wr=0.5_size=4_i=1_j=2",
      "fgnet_bag_wr=0.5_size=4_i=1_j=3",
      "fgnet_bag_wr=0.5_size=4_i=1_j=4",
      "fgnet_bag_wr=0.5_size=4_i=2_j=0",
      "fgnet_bag_wr=0.5_size=4_i=2_j=1",
      "fgnet_bag_wr=0.5_size=4_i=2_j=2",
      "fgnet_bag_wr=0.5_size=4_i=2_j=3",
      "fgnet_bag_wr=0.5_size=4_i=2_j=4",
      "fgnet_bag_wr=0.5_size=4_i=3_j=0",
      "fgnet_bag_wr=0.5_size=4_i=3_j=1",
      "fgnet_bag_wr=0.5_size=4_i=3_j=2",
      "fgnet_bag_wr=0.5_size=4_i=3_j=3",
      "fgnet_bag_wr=0.5_size=4_i=3_j=4",
      "fgnet_bag_wr=0.5_size=4_i=4_j=0",
      "fgnet_bag_wr=0.5_size=4_i=4_j=1",
      "fgnet_bag_wr=0.5_size=4_i=4_j=2",
      "fgnet_bag_wr=0.5_size=4_i=4_j=3",
      "fgnet_bag_wr=0.5_size=4_i=4_j=4",
  ],
  "batch_size": [1],
  "learning_rate": [0.01, 0.001, 0.0001],
  "epochs": [100],
  "pooling_mode": ["max", "mean"],
  "early_stopping": [False],
}
```

## Results

Showing results from the following experiments: `r exp_list`

### Anova modeling

```{r}
# Fit models 


# Use weights to balance the data by data set type
mod_weight <- df_metrics %>% 
  filter(experiment %in% exp_list) %>% 
  count(data_set_type) %>% 
  mutate(weight = min(n) / n) %>% 
  select(data_set_type, weight) %>% 
  deframe()


models <- 
  expand_grid(
    outcome = c("mae", "acc", "rmse"),
    predictors = c("ordinal_method*mil_method + (1 | data_set_name) + pooling_mode + learning_rate"),
    # predictors = c("ordinal_method*mil_method + data_set_type + (1 | data_set_name) + pooling_mode + learning_rate"),
    exp_list = list(exp_list)
  ) %>% 
  rowwise() %>% 
  mutate(
    df = list(filter(df_metrics, experiment %in% exp_list)), 
    w = list(mod_weight[df$data_set_type]), 
    formula = list(as.formula(glue("{outcome} ~ {predictors}"))),
    fit = list(lmer(formula, df, weights = w)),
    aov2 = list(car::Anova(fit, type = 2)),
    aov3 = list(car::Anova(fit, type = 3))
  )

```

From this experiment, the ordinal method, pooling mode, and interaction between ordinal method and MIL method show up as significant factors in the model

```{r}
#| label: out-anova-results
walk(models$aov2, print)
```

```{r}
plot_one_factor <- function(df, exps, factor, metric, facets = NULL) {
  df %>% 
    filter(experiment %in% exps) %>% 
    ggplot(aes_(
      x = as.name(factor),
      y = as.name(metric)
    )) +
    # geom_violin(width = 0.5, draw_quantiles = c(0.25, 0.5, 0.75)) + 
    geom_boxplot(width = 0.5) + 
    stat_summary(geom = "point", fun = mean, size = 3) +
    facet_wrap(facets, scales="free") + 
    theme_bw() +
    coord_flip()
}
```

```{r}
#| label: fig-1f-boxplot-mae-ordinal
#| fig-cap: Main single factor effects with MAE outcome
#| fig-height: 6
plot_one_factor(df_metrics, exp_list, "ordinal_method", "mae", facets = "data_set_type") + 
  plot_one_factor(df_metrics, exp_list, "mil_method", "mae", facets = "data_set_type") +
  plot_one_factor(df_metrics, exp_list, "pooling_mode", "mae", facets = "data_set_type") +
  plot_layout(ncol = 1)
```

```{r}
#| label: fig-1f-boxplot-acc-ordinal
#| fig-cap: Main single factor effects with accuracy outcome
#| fig-height: 6
plot_one_factor(df_metrics, exp_list, "ordinal_method", "acc", facets = "data_set_type") + 
  plot_one_factor(df_metrics, exp_list, "mil_method", "acc", facets = "data_set_type") +
  plot_one_factor(df_metrics, exp_list, "pooling_mode", "acc", facets = "data_set_type") +
  plot_layout(ncol = 1)
```


```{r}
plot_two_factors <- function(
    df, 
    exps, 
    factors = c("ordinal_method", "mil_method"),
    metric = "mae", 
    facets = NULL) {
  
  df %>% 
    filter(experiment %in% exps) %>% 
    ggplot(aes_(
      x = as.name(factors[1]), 
      y = as.name(metric), 
      color = as.name(factors[2])
    )) +
    geom_boxplot() +
    stat_summary(geom = "point", 
                 position = position_dodge2(width = 0.75),
                 fun = mean, size = 5) +
    facet_wrap(facets, scales="free") + 
    theme_bw()
}
```

```{r}
#| label: fig-2f-boxplot-mae-mil_ordinal
#| fig-cap: Two factor effects of MIL and Ordinal method with MAE outcome
plot_two_factors(df_metrics, exp_list, metric = "mae", facets = "data_set_type")
```

```{r}
#| label: fig-2f-boxplot-acc-mil_ordinal
#| fig-cap: Two factor effects of MIL and Ordinal method with accuracy outcome
plot_two_factors(df_metrics, exp_list, metric = "acc", facets = "data_set_type")
```

```{r}
plot_interaction <- function(
    df, 
    exps, 
    factors = c("ordinal_method", "mil_method"), 
    metric = "mae") {
  
  df %>% 
    filter(experiment %in% exps) %>% 
    ggplot(aes_(
      x = as.name(factors[1]), 
      y = as.name(metric), 
      color = as.name(factors[2]))
    ) + 
    geom_quasirandom(alpha = 0.5) +
    stat_summary(geom = "point", fun = mean, size = 5) +
    stat_summary(aes_(group = as.name(factors[2])), geom = "line", fun = mean) +
    theme_minimal()
}

```

```{r}
#| label: fig-interaction-mae-1.0.1
#| fig-cap: Interaction plot between MIL and Ordinal method with MAE outcome
#| warning: false

plot_interaction(df_metrics, exp_list, metric = "mae") +
  facet_wrap(~data_set_type, scales = "free")
```

```{r}
#| label: fig-interaction-acc-1.0.1
#| fig-cap: Interaction plot between MIL and Ordinal method with accuracy outcome
#| warning: false
plot_interaction(df_metrics, exp_list, metric = "acc") +
  facet_wrap(~data_set_type, scales = "free")
```

```{r}
#| label: fig-interaction-rmse-1.0.1
#| fig-cap: Interaction plot between MIL and Ordinal method with RMSE outcome
#| warning: false
plot_interaction(df_metrics, exp_list, metric = "rmse") +
  facet_wrap(~data_set_type, scales = "free")
```

### Training and validation across epochs

```{r}
plot_training_progression <- function(df, exps, metric, group, alpha = 0.1) {
  
  df %>% 
    filter(experiment %in% exps) %>% 
    ggplot(aes(
      x = epoch, 
      y = {{ metric }},
      group = file, 
      color = {{ group }} # paste0(ordinal_method, " ", mil_method))
    )) + 
    geom_line(alpha = alpha) +
    scale_y_log10() +
    scale_color_brewer(name = NULL, palette = "Paired") + 
    guides(colour = guide_legend(override.aes = list(alpha = 1))) + 
    theme_minimal() 
}
```

```{r}
plot_training_progression(df_train_prog, c("fgnet-1.0.1", "fgnet-1.0.2"), 
                          loss, paste0(mil_method, " ", ordinal_method))
```

Observations:

-   Training loss drops down in the first 5-10 epochs, and then sometimes goes back up. This trend is especially prevalent for the CORN methods.
-   NOTE: CORN and CORAL training losses may be on different scales, so it is important not to compare them against each other
-   Need to check whether the learning rate is impacting how fast the model learns (see first few epochs of CORAL models)
-   Probably safe to reduce the number of epochs used based on this assessment.
-   Notably, the deep supervision doesn't seem to be doing as well, but it a lot more stable

```{r}
#| warning: false
plot_training_progression(df_train_prog, c("fgnet-1.0.1", "fgnet-1.0.2"),  
                          mean_absolute_error_labels, paste0(ordinal_method, " ", mil_method))
```

```{r}
#| fig-width: 8
#| fig-height: 6
plot_training_progression(df_train_prog, c("fgnet-1.0.1", "fgnet-1.0.2"),
                          val_loss, paste0(ordinal_method, " ", mil_method)) + 
  facet_wrap(facets = vars(ordinal_method, mil_method))

```

The above plot (although it is noisy) indicates to me that the model isn't overfitting. The validation loss is mostly constant throughout the epochs.

```{r}
#| eval: false
plot_training_progression(df_train_prog, c("aes-2.0.1"), 
                          loss, paste0(mil_method, " ", ordinal_method),
                          alpha = 0.5)
```

```{r}
#| eval: false
exp_list <- c("aes-2.0.1")
df_train_prog %>% 
  filter(experiment %in% exp_list) %>% 
  group_by(file) %>% 
  summarize(n_epoch = max(epoch)) %>% 
  ggplot(aes(n_epoch)) +
  geom_bar() +
  scale_x_continuous(limits = c(0, NA)) + 
  theme_minimal() + 
  labs(
    x = "Number of epochs",
    y = "Count",
    title = glue("Early stopping summary for {paste0(exp_list, collapse = ', ')}")
  )
```

## Next actions

## Appendix

Linear model diagnostics are reasonable. See some deviations from normality and possible heterogeneity in the residuals. Accuracy model is the model reliable in this.

```{r}
walk(models$fit, ~print(plot(.x)))

```

Correlation between number of unique test predictions and outcome metrics

```{r}
exp_list <- c("fgnet-1.0.1", "fgnet-1.0.2", "aes-2.0.1", "aes-2.0.2")
df_metrics %>% 
  filter(experiment %in% exp_list) %>% 
  ggplot(aes(n_pred, mae, group = n_pred)) + 
  geom_boxplot() + 
  facet_wrap(~data_set_type, scales = "free") +
  theme_bw()

df_metrics %>% 
  filter(experiment %in% exp_list) %>% 
  ggplot(aes(n_pred, acc, group = n_pred)) + 
  geom_boxplot() + 
  facet_wrap(~data_set_type, scales = "free") +
  theme_bw()

```

```{r}

files_of_interest <- 
  df_metrics %>% 
  filter(experiment %in% c("aes-2.0.1", "aes-2.0.2")) %>% 
  filter(n_pred > 1) %>% 
  pull(file)

files_of_interest <- str_replace_all(files_of_interest, ".csv", "_training\\.log")

df_train_prog %>% 
  filter(file %in% files_of_interest) %>% 
  group_by(file) %>% 
  summarize(n_epoch = max(epoch)) 
```
